{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Aadhaar Data Analytics Project\n",
                "\n",
                "## Objective\n",
                "Analyze the gap between registered Aadhaar holders and actively participating users to estimate inactivity or utilization patterns. We aim to produce neutral, policy-oriented insights regarding enrolment vs. authentication levels across states.\n",
                "\n",
                "## Scope & Limitations\n",
                "- **Inactivity Index**: Framed as a comparative indicator of engagement (Authentication / Enrolment volume) rather than absolute dormancy.\n",
                "- **Failure Rates**: Due to data limitations (lack of explicit failure counts), analysis focuses on *utilization volume*.\n",
                "- **Demographic Data**: Used as a proxy for non-biometric authentication/update activity.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0e633859",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import scipy.stats\n",
                "from sklearn.ensemble import IsolationForest\n",
                "import glob\n",
                "import os\n",
                "\n",
                "# Set aesthetic style\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "52523e52",
            "metadata": {},
            "source": [
                "# 1. Data Preparation\n",
                "\n",
                "## 1.1 Load Data\n",
                "The data is distributed across three folders (`biometric`, `demographic`, `enrolment`), each containing chunked CSV files. We will load and combine them into three master DataFrames."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d12e3538",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset(folder_path):\n",
                "    \"\"\"\n",
                "    Reads all CSV files in the given folder and concatenates them.\n",
                "    \"\"\"\n",
                "    all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
                "    if not all_files:\n",
                "        print(f\"No files found in {folder_path}\")\n",
                "        return pd.DataFrame()\n",
                "    \n",
                "    df_list = []\n",
                "    for filename in all_files:\n",
                "        try:\n",
                "            df = pd.read_csv(filename)\n",
                "            df_list.append(df)\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading {filename}: {e}\")\n",
                "            \n",
                "    return pd.concat(df_list, ignore_index=True) if df_list else pd.DataFrame()\n",
                "\n",
                "# Define paths based on workspace structure\n",
                "base_path = \"/Users/shikhar/Downloads/Shikhar/hacka/Dataset\"\n",
                "bio_path = os.path.join(base_path, \"api_data_aadhar_biometric\")\n",
                "demo_path = os.path.join(base_path, \"api_data_aadhar_demographic\")\n",
                "enrol_path = os.path.join(base_path, \"api_data_aadhar_enrolment\")\n",
                "\n",
                "print(\"Loading Biometric Data...\")\n",
                "df_bio = load_dataset(bio_path)\n",
                "print(f\"Biometric Shape: {df_bio.shape}\")\n",
                "\n",
                "print(\"Loading Demographic Data...\")\n",
                "df_demo = load_dataset(demo_path)\n",
                "print(f\"Demographic Shape: {df_demo.shape}\")\n",
                "\n",
                "print(\"Loading Enrolment Data...\")\n",
                "df_enrol = load_dataset(enrol_path)\n",
                "print(f\"Enrolment Shape: {df_enrol.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4869349d",
            "metadata": {},
            "source": [
                "## 1.2 Data Standardization\n",
                "We need to ensure state names are consistent across all datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "482265e9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standardization of State Names\n",
                "def standardize_states(df, state_col='state'):\n",
                "    if df.empty:\n",
                "        return df\n",
                "    \n",
                "    # Common mappings for Indian states\n",
                "    state_map = {\n",
                "        'Orissa': 'Odisha',\n",
                "        'Pondicherry': 'Puducherry',\n",
                "        'Delhi': 'NCT of Delhi',\n",
                "        'Andaman and Nicobar Islands': 'Andaman & Nicobar Islands',\n",
                "        'Jammu and Kashmir': 'Jammu & Kashmir',\n",
                "        'Dadra and Nagar Haveli': 'Dadra & Nagar Haveli and Daman & Diu',\n",
                "        'Daman and Diu': 'Dadra & Nagar Haveli and Daman & Diu',\n",
                "        # Catch any casing issues\n",
                "        'Telengana': 'Telangana'\n",
                "    }\n",
                "    \n",
                "    df[state_col] = df[state_col].astype(str).str.strip()\n",
                "    df[state_col] = df[state_col].replace(state_map)\n",
                "    return df\n",
                "\n",
                "df_bio = standardize_states(df_bio)\n",
                "df_demo = standardize_states(df_demo)\n",
                "df_enrol = standardize_states(df_enrol)\n",
                "\n",
                "# Check unique states\n",
                "print(\"Unique States in Biometric:\", df_bio['state'].nunique())\n",
                "print(\"Unique States in Enrolment:\", df_enrol['state'].nunique())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f5c1f5ce",
            "metadata": {},
            "source": [
                "# 2. Analysis\n",
                "\n",
                "## 2.1 Aggregation by State\n",
                "We aggregate the counts by state to perform high-level analysis. We assume that the files contain incremental or snapshot data that should be summed to get the total volume for the period."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e7cb4397",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter numeric columns for aggregation (excluding pincode)\n",
                "def get_sum_columns(df, prefix):\n",
                "    return [c for c in df.columns if c.startswith(prefix)]\n",
                "\n",
                "bio_cols = get_sum_columns(df_bio, 'bio_age')\n",
                "demo_cols = get_sum_columns(df_demo, 'demo_age')\n",
                "enrol_cols = get_sum_columns(df_enrol, 'age_')\n",
                "\n",
                "print(f\"Biometric Cols to sum: {bio_cols}\")\n",
                "print(f\"Demographic Cols to sum: {demo_cols}\")\n",
                "print(f\"Enrolment Cols to sum: {enrol_cols}\")\n",
                "\n",
                "# Aggrgation\n",
                "state_bio = df_bio.groupby('state')[bio_cols].sum().reset_index()\n",
                "state_demo = df_demo.groupby('state')[demo_cols].sum().reset_index()\n",
                "state_enrol = df_enrol.groupby('state')[enrol_cols].sum().reset_index()\n",
                "\n",
                "# Calculate Totals\n",
                "state_bio['Total_Biometric'] = state_bio[bio_cols].sum(axis=1)\n",
                "state_demo['Total_Demographic'] = state_demo[demo_cols].sum(axis=1)\n",
                "state_enrol['Total_Enrolment'] = state_enrol[enrol_cols].sum(axis=1)\n",
                "\n",
                "# Merge datasets\n",
                "df_master = pd.merge(state_enrol[['state', 'Total_Enrolment']], state_bio[['state', 'Total_Biometric']], on='state', how='outer')\n",
                "df_master = pd.merge(df_master, state_demo[['state', 'Total_Demographic']], on='state', how='outer')\n",
                "\n",
                "# Fill NaNs with 0 (assuming missing means no activity in that state for that category)\n",
                "df_master = df_master.fillna(0)\n",
                "\n",
                "# Total Authentications\n",
                "df_master['Total_Authentications'] = df_master['Total_Biometric'] + df_master['Total_Demographic']\n",
                "\n",
                "df_master.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ea0d05f7",
            "metadata": {},
            "source": [
                "## 2.2 Core Metrics Calculation\n",
                "\n",
                "### Inactivity Index\n",
                "**Formula**: $1 - \\frac{\\text{Total Authentications}}{\\text{Total Enrolment}}$\n",
                "\n",
                "> **Interpretation**: \n",
                "> * Values closer to **1** indicate high inactivity (low utilization relative to enrolment).\n",
                "> * Values < **0** indicate utilization exceeds the captured enrolment flow (highly active). \n",
                "> * Values near **0** indicate balanced activity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5cf728bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inactivity Index\n",
                "df_master['Inactivity_Index'] = 1 - (df_master['Total_Authentications'] / df_master['Total_Enrolment'])\n",
                "\n",
                "# Update Activity Rate (Demographic share of total auth)\n",
                "# This acts as a proxy for \"Correction/Update\" vs \"Usage\"\n",
                "df_master['Demo_Auth_Share'] = df_master['Total_Demographic'] / df_master['Total_Authentications']\n",
                "\n",
                "# Handle edge cases (divide by zero)\n",
                "df_master.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "\n",
                "df_master.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "79581feb",
            "metadata": {},
            "source": [
                "# 3. Exploratory Data Analysis (EDA)\n",
                "\n",
                "## 3.1 State Rankings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "20a47ccb",
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_top_bottom(df, col, title, n=5):\n",
                "    sorted_df = df.sort_values(col)\n",
                "    top = sorted_df.tail(n)\n",
                "    bottom = sorted_df.head(n)\n",
                "    combined = pd.concat([bottom, top])\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    sns.barplot(data=combined, y='state', x=col, hue='state', palette='coolwarm', legend=False)\n",
                "    plt.title(f\"Top and Bottom {n} States by {title}\")\n",
                "    plt.axvline(0, color='black', linewidth=1)\n",
                "    plt.show()\n",
                "\n",
                "plot_top_bottom(df_master, 'Inactivity_Index', 'Inactivity Index (Higher = Less Active)')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dbbab8ca",
            "metadata": {},
            "source": [
                "## 3.2 Enrolment vs Authentication Volume\n",
                "Comparing the raw usage volume against the enrolled base."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ce8fe21f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot for a subset of large states to avoid clutter\n",
                "# We pick top 10 by enrolment volume\n",
                "top_states = df_master.nlargest(10, 'Total_Enrolment')\n",
                "\n",
                "top_states_melt = top_states.melt(id_vars='state', \n",
                "                                  value_vars=['Total_Enrolment', 'Total_Authentications'], \n",
                "                                  var_name='Metric', value_name='Count')\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "    sns.barplot(data=top_states_melt, x='state', y='Count', hue='Metric')\n",
                "plt.title(\"Enrolment vs Authentication in Top 10 Enrolled States\")\n",
                "plt.xticks(rotation=45)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Anomaly Detection\n",
                "\n",
                "We use **Z-Score** to identify states that deviate significantly from the national average Inactivity Index.\n",
                "- **Z > 2**: Significantly High Inactivity (Potential ghost beneficiaries or migration OUT).\n",
                "- **Z < -2**: Significantly High Activity (Potential migration IN or high service dependency)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mean_inactivity = df_master['Inactivity_Index'].mean()\n",
                "std_inactivity = df_master['Inactivity_Index'].std()\n",
                "\n",
                "df_master['Z_Score'] = (df_master['Inactivity_Index'] - mean_inactivity) / std_inactivity\n",
                "\n",
                "anomalies = df_master[(df_master['Z_Score'] > 2) | (df_master['Z_Score'] < -2)]\n",
                "\n",
                "print(\"Detected Anomalies:\")\n",
                "display(anomalies[['state', 'Total_Enrolment', 'Total_Authentications', 'Inactivity_Index', 'Z_Score']])\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.scatterplot(data=df_master, x='Total_Enrolment', y='Inactivity_Index', hue='Z_Score', palette='RdBu_r', size='Z_Score')\n",
                "plt.title(\"Anomaly Detection: Inactivity vs Enrolment Size\")\n",
                "plt.axhline(mean_inactivity, color='gray', linestyle='--')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5. Policy Recommendations\n",
                "\n",
                "Based on the analysis:\n",
                "\n",
                "1. **High Inactivity States**:\n",
                "    - States with high Z-scores require targeted **Aadhaar usage drives**.\n",
                "    - Investigation into **dead/duplicate entries** is recommended if demographic updates provided are also low.\n",
                "    \n",
                "2. **High Utilization States**:\n",
                "    - States with negative Inactivity Indices (High utilization) likely have **migrant inflows** or heavy reliance on DBT (Direct Benefit Transfer).\n",
                "    - Recommendation: Strengthen **authentication infrastructure** (server capacity, biometric devices) in these regions to prevent failures.\n",
                "    \n",
                "3. **Demographic vs Biometric**:\n",
                "    - Regions with disproportionately high Demographic authentication may be facing **biometric failures** (e.g., manual override or OTP fallback). Targeted hardware audits are advised."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
